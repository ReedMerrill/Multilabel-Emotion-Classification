{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 12:51:24.299651: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-04 12:51:24.302542: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 12:51:24.335800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-04 12:51:24.335834: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-04 12:51:24.335867: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-04 12:51:24.342646: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 12:51:24.343067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 12:51:25.391069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import numpy\n",
    "\n",
    "train_path=\"data/train.csv\"\n",
    "dev_path=\"data/dev.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/reed/.cache/huggingface/datasets/csv/default-0989c6c4d599a70a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47904d779b7445d9241eb7be2c942eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b73331996b547c48dbe7e564b27d254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90d9468e2074a2a8799a072627ec024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bf57accc2343fa8ceff901b2a82e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad51fcc982e43cca9b2c550f48f0f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the tokenizer from DistilRoBERTa\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\", pad_token_id=0)\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"Converts the text of each example to \"input_ids\", a sequence of integers\n",
    "    representing 1-hot vectors for each token in the text\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=64,\n",
    "                     padding=\"max_length\")\n",
    "\n",
    "# load the CSVs into Huggingface datasets to allow use of the tokenizer\n",
    "hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "    \"train\": train_path, \"validation\": dev_path})\n",
    "\n",
    "# the labels are the names of all columns except the first\n",
    "labels = hf_dataset[\"train\"].column_names[1:]\n",
    "\n",
    "def gather_labels(example):\n",
    "    \"\"\"Converts the label columns into a list of 0s and 1s\"\"\"\n",
    "    # the float here is because converting hf to tf data requires a list or array of labels\n",
    "    return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "# convert text and labels to format expected by model\n",
    "hf_dataset = hf_dataset.map(gather_labels)\n",
    "hf_dataset = hf_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# split train and val into their own objects\n",
    "hf_train = hf_dataset['train']\n",
    "hf_dev = hf_dataset['validation']\n",
    "\n",
    "# rename column for embeddings layer\n",
    "hf_train = hf_train.rename_column(\"input_ids\", \"embedding_inputs\")\n",
    "hf_dev = hf_dev.rename_column(\"input_ids\", \"embedding_inputs\")\n",
    "\n",
    "# convert Huggingface datasets to Tensorflow datasets\n",
    "train_dataset = hf_train.to_tf_dataset(\n",
    "    columns=\"embedding_inputs\",\n",
    "    label_cols=\"labels\",\n",
    "    batch_size=32,\n",
    "    shuffle=True)\n",
    "dev_dataset = hf_dev.to_tf_dataset(\n",
    "    columns=\"embedding_inputs\",\n",
    "    label_cols=\"labels\",\n",
    "    batch_size=32)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# extract labels as a numpy array\n",
    "train_unbatched = train_dataset.unbatch()\n",
    "labels_tmp = np.asarray(list(train_unbatched.map(lambda x, y: y)))\n",
    "print(labels_tmp)\n",
    "# convert to pandas\n",
    "labels_df = pd.DataFrame(labels_tmp)\n",
    "# convert labels to a string of ids\n",
    "labels_df['ids'] = pd.DataFrame(labels_df.astype(int).astype(str).agg(''.join, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0000000': 7.142346975216056e-05, '0000001': 0.0019646365422396855, '0000010': 0.007407407407407408, '0000100': 0.012658227848101266, '0001000': 0.0005580357142857143, '0001001': 0.3333333333333333, '0001010': 0.5, '0010000': 0.000449034575662326, '0010001': 0.07142857142857142, '0010010': 0.1111111111111111, '0010100': 0.125, '0011000': 0.023255813953488372, '0100000': 0.00047664442326024784, '0100001': 0.1111111111111111, '0100010': 0.5, '0100100': 1.0, '0101000': 0.0196078431372549, '0101001': 1.0, '0110000': 0.012658227848101266, '0110010': 1.0, '0111000': 0.5, '1000000': 0.0002817695125387433, '1000001': 0.125, '1000010': 0.25, '1000100': 0.043478260869565216, '1001000': 0.00546448087431694, '1010000': 0.003703703703703704, '1010001': 1.0, '1011000': 0.125, '1100000': 0.012048192771084338, '1101000': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# get the number of times each id appears\n",
    "weights = labels_df.groupby('ids').size().reset_index(name='n')\n",
    "# calculate weights\n",
    "weights['weight'] = (1 / weights['n'])\n",
    "# convert to dictionary\n",
    "weights = weights.set_index('ids')['weight'].to_dict()\n",
    "print(weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights-and-masks_time_1701719529.3200686\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# fit the model to the training data, monitoring F1 on the dev data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mdev_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     class_weight\u001b[39m=\u001b[39;49mweights_array,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         ModelCheckpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             filepath\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoints/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_f1_score\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m             save_best_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         TensorBoard(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             log_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogs/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         EarlyStopping(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_f1_score\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             min_delta\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m             start_from_epoch\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m time_elapsed \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time) \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reed/Projects/neural-networks/graduate-project-ReedMerrill/nn.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime Elapsed: \u001b[39m\u001b[39m{\u001b[39;00mtime_elapsed\u001b[39m}\u001b[39;00m\u001b[39m min.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnets/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/nnets/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1327\u001b[0m, in \u001b[0;36mDataHandler._configure_dataset_and_inferred_steps\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mdel\u001b[39;00m x\n\u001b[1;32m   1326\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter\u001b[39m.\u001b[39mget_dataset()\n\u001b[0;32m-> 1327\u001b[0m \u001b[39mif\u001b[39;00m class_weight:\n\u001b[1;32m   1328\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(_make_class_weight_map_fn(class_weight))\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_steps(steps_per_epoch, dataset)\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# define grid search parameters and loop\n",
    "\n",
    "# define a model with a single fully connected layer\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=tokenizer.vocab_size,\n",
    "    output_dim=16,\n",
    "    mask_zero=True))\n",
    "model.add(layers.Bidirectional(layers.GRU(64)))\n",
    "# final processing with a dense RELU layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(\n",
    "    units=len(labels),\n",
    "    activation='sigmoid'))\n",
    "\n",
    "# specify compilation hyperparameters\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=0.5)])\n",
    "\n",
    "# set time for run time and model naming\n",
    "start_time = time.time()\n",
    "desc = \"weights-and-masks\"\n",
    "model_name = f\"{desc}_time_{start_time}\"\n",
    "\n",
    "# fit the model to the training data, monitoring F1 on the dev data\n",
    "print(model_name)\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=dev_dataset,\n",
    "    class_weight=weights_array,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            filepath=f\"checkpoints/{model_name}\",\n",
    "            monitor=\"val_f1_score\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True),\n",
    "        TensorBoard(\n",
    "            log_dir=f\"logs/{model_name}\"),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_f1_score\",\n",
    "            min_delta=0.25,\n",
    "            patience=5,\n",
    "            start_from_epoch=2)\n",
    "        ])\n",
    "\n",
    "time_elapsed = (time.time() - start_time) / 60\n",
    "print(f\"Time Elapsed: {time_elapsed} min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "model_path=\"checkpoints/batch-size_32_time_1701555125.2780304\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "df = pd.read_csv(dev_path)\n",
    "\n",
    "# generate predictions from model (on the tf version of validation data)\n",
    "predictions = numpy.where(model.predict(dev_dataset) > 0.5, 1, 0)\n",
    "\n",
    "# assign predictions to label columns in Pandas data frame\n",
    "df.iloc[:, 1:] = predictions\n",
    "print(df.head())\n",
    "\n",
    "# write the Pandas dataframe to a zipped CSV file\n",
    "df.to_csv(\"submission.zip\", index=False, compression=dict(\n",
    "    method='zip', archive_name=f'submission.csv'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
